{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\anaconda3\\envs\\cuda121-2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", use_fast=False, trust_remote_code=True)\n",
    "# 加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", device_map=\"cuda\", torch_dtype=\"auto\")\n",
    "\n",
    "# 加载 LoRA 适配器权重\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id=\"./output/binary-class0113/checkpoint-2000/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('eval-binary.json')\n",
    "\n",
    "def predict(messages, model, tokenizer):\n",
    "    device = \"cuda\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # print(response)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7734 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "d:\\Programs\\anaconda3\\envs\\cuda121-2\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:580: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 7734/7734 [20:11<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7848461339539695\n",
      "tp:3763, tn:2307, fp:1, fn:1663\n",
      "f1_score:  0.8189336235038085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prompt = '''在这个任务中，你是一位资深的反诈骗网络安全分析师，你的职责是利用你的专业知识和对网络诈骗行为的深刻理解，从短信文本中识别出可能存在的欺诈行为和风险类别。你的工作对于提前预警潜在的网络诈骗，保护用户财产安全和个人信息不被侵犯具有重要意义。现在，请仔细审查以下短信文本，并运用你的专业判断该短信是否有风险，回答“无风险”或“有风险”'''\n",
    "\n",
    "tp, tn, fp, fn = 0,0,0,0\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    input_value = row['文本']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    # print(messages)\n",
    "    response = predict(messages, model, tokenizer)\n",
    "    answer = row['风险类别']\n",
    "    if response.strip() == answer:\n",
    "        if response.strip()[0] == \"无\":\n",
    "            tn += 1\n",
    "        elif response.strip()[0] == \"有\":\n",
    "            tp += 1\n",
    "    elif response.strip()[0] == \"无\" and answer[0] == \"有\":\n",
    "        fn += 1\n",
    "    elif response.strip()[0] == \"有\" and answer[0] == \"无\":\n",
    "        fp += 1\n",
    "        \n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (tp + tn) / test_df.shape[0]\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"tp:%d, tn:%d, fp:%d, fn:%d\"%(tp, tn, fp, fn))\n",
    "print(\"f1_score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7734/7734 [19:54<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9013447116627877\n",
      "tp:4664, tn:2307, fp:1, fn:762\n",
      "f1_score:  0.9243880685759588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", use_fast=False, trust_remote_code=True)\n",
    "# 加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", device_map=\"cuda\", torch_dtype=\"auto\")\n",
    "\n",
    "# 加载 LoRA 适配器权重\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id=\"./output/binary-class0113/checkpoint-4000/\",\n",
    ")\n",
    "\n",
    "tp, tn, fp, fn = 0,0,0,0\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    input_value = row['文本']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    # print(messages)\n",
    "    response = predict(messages, model, tokenizer)\n",
    "    answer = row['风险类别']\n",
    "    if response.strip() == answer:\n",
    "        if response.strip()[0] == \"无\":\n",
    "            tn += 1\n",
    "        elif response.strip()[0] == \"有\":\n",
    "            tp += 1\n",
    "    elif response.strip()[0] == \"无\" and answer[0] == \"有\":\n",
    "        fn += 1\n",
    "    elif response.strip()[0] == \"有\" and answer[0] == \"无\":\n",
    "        fp += 1\n",
    "        \n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (tp + tn) / test_df.shape[0]\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"tp:%d, tn:%d, fp:%d, fn:%d\"%(tp, tn, fp, fn))\n",
    "print(\"f1_score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7734/7734 [19:39<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8797517455391777\n",
      "tp:4497, tn:2307, fp:1, fn:929\n",
      "f1_score:  0.9062877871825876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", use_fast=False, trust_remote_code=True)\n",
    "# 加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Qwen2.5-0.5B-Instruct\", device_map=\"cuda\", torch_dtype=\"auto\")\n",
    "\n",
    "# 加载 LoRA 适配器权重\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id=\"./output/binary-class0113/checkpoint-6000/\",\n",
    ")\n",
    "\n",
    "tp, tn, fp, fn = 0,0,0,0\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    input_value = row['文本']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    # print(messages)\n",
    "    response = predict(messages, model, tokenizer)\n",
    "    answer = row['风险类别']\n",
    "    if response.strip() == answer:\n",
    "        if response.strip()[0] == \"无\":\n",
    "            tn += 1\n",
    "        elif response.strip()[0] == \"有\":\n",
    "            tp += 1\n",
    "    elif response.strip()[0] == \"无\" and answer[0] == \"有\":\n",
    "        fn += 1\n",
    "    elif response.strip()[0] == \"有\" and answer[0] == \"无\":\n",
    "        fp += 1\n",
    "        \n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (tp + tn) / test_df.shape[0]\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"tp:%d, tn:%d, fp:%d, fn:%d\"%(tp, tn, fp, fn))\n",
    "print(\"f1_score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
